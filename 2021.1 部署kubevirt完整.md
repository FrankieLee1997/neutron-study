### 成熟方案，可参考

1. Openstack（不想用）

   ![KVMç³»å06ãNova éè¿libvirt ç®¡çQEMU/KVM èæº_Javaä»åèµ°å¤©æ¶¯-CSDNåå®¢](https://images0.cnblogs.com/blog2015/697113/201506/110959507542691.jpg)

2. OpenShift的混合workloads（use OpenShift Virtualization to deploy Virtual Machines and containers side by side to run your hybrid workloads）

   https://www.openshift.com/blog/containers-and-virtual-machines-together-on-red-hat-openshift-platform

   对网络功能的支持不如neutron，查到的资料较少，工作类似一期比较繁琐。

### 我们可以尝试的方案

1. 容器部分，保持smartkeeper对k8s的相关接口的调用，维持监控方案；

2. 虚拟机部分，使用libvirt完成一套对其启动/销毁/监控的接口，并写入smartkeeper前端（一套新client，具备可行性）。

   ![Look Into Libvirt Osier Yang](https://image.slidesharecdn.com/lookintolibvirtosieryang-1354066036617-phpapp02-121127193044-phpapp02/95/look-into-libvirt-osier-yang-16-638.jpg?cb=1354044692)

3. kubevirt：创建了一个特殊的pod `virt-launcher` 其中的子进程包括`libvirt`和`qemu`

   ![img](https://remimin.github.io/img/2018-09-14-kubevirt/architecture.png)

### 问题

1. 容器网络管理方面，不论是calico、sdn控制器应该都具可行性；虚拟机方面，做一套类似下图nova api和neutronclient的可行性；

   ![image-20210105181116758](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210105181116758.png)

2. 对虚拟机的数量级要求，服务器配置和对其管理的可靠性；如果简单启动少数实例，直接用kvm默认的网络方案（如下图），解决与docker网络冲突即可，不单独写一套管理kvm网络和镜像的组件。

   ![u'i](https://imgconvert.csdnimg.cn/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI5MTA0OTEzNzU2?x-oss-process=image/format,png)

   kvm的4种网络方案：https://blog.csdn.net/gsl371/article/details/78662258

   ------

   ***docker的桥接和kvm桥接的冲突*，可能出现类似“启动docker导致kvm虚拟机断网”的问题（http://blog.chinaunix.net/uid-16361381-id-5823762.html）解决docker0和virbr0与物理机网卡的冲突





# 升级

192、193：docker 19.3.11，kubeadm、kubectl、kubelet 1.19.2

calico v3.17 cidr：172.10.0.0/16

kubevirt 0.36.0 https://www.cnblogs.com/ryanyangcs/p/14079144.html

calicoctl 3.17.1

cdi 1.28.0 

`Containerized Data Importer`（CDI）项目提供了用于使 PVC （file system）作为 KubeVirt VM 磁盘的功能。建议同时部署 CDI：

virtctl 0.36.0

# 基础vm部署

https://raw.githubusercontent.com/kubevirt/kubevirt.github.io/master/labs/manifests/vm.yaml

官方的registry中的cirros、fedora

基础镜像可apply，环境没问题

![image-20210112150213431](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112150213431.png)

ping一下，没问题：

![image-20210112163516173](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112163516173.png)

现在再看kubevirt架构：

![img](https://pic4.zhimg.com/80/v2-959d2a9d97d85a95b4596d4650cd3477_720w.jpg)

Virt-api：kubevirt是以CRD形式去管理vm pod， virt-api就是所有虚拟化操作的入口，包括常规的CRD更新验证以及vm start、stop，kubevirt用kubevirt.io形式实现了crd的接口定义

如下是创建一个VMI对象的API调用：

```
/apis/kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm
```


而通过子资源方式来访问图形化VNC的API调用是这样的：

```
/apis/subresources.kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm
```

Virt-handler会以Daemonset形式部署在每个节点上，负责监控节点上每个虚拟机实例状态变化：

![image-20210112161503527](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112161503527.png)

可以看到后台挂载进程的virt-handler，挂在193，发送metric请求，8186端口

![image-20210112160756011](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112160756011.png)

virt-lanuncher pod对应着一个VMI， kubelet只是负责virt-lanuncher pod运行状态，不会去关心VMI创建情况：

![image-20210112161124740](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112161124740.png)

virt-handler会根据CRD参数配置去通知virt-lanuncher去使用本地libvirtd实例来启动VMI，每个virt-lanuncher pod对应一个libvirtd，virt-lanuncher通过libvirtd去管理VM的生命周期，这样做到去中心化，不再是以前虚拟机那套做法，一个libvirtd去管理多个VM。

本地libvirt进程，virt-launcher pod调用一个对应libvirtd，manufacturer为KubeVirt：

![image-20210112161325897](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112161325897.png)

看看docker，和容器形式的部署有些类似，多了一个磁盘方面的docker支持，剩下两个关联着virt-launcher pod的容器：

![image-20210112161659135](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112161659135.png)

# 基础vm迁移

通过CRD： VirtualMachineInstanceMigration（VMIM） 来实现动态迁移

```text
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachineInstanceMigration
metadata:
  name: migration-job
spec:
  vmiName: vmi-fedora
```

这种迁移形式其实并没有指定迁移到哪些节点上，内部逻辑只是重新调度virt-lanucher pod， 其中kubeirt-config可以对此进行迁移的相关限制，比如迁移的频率（同时只能几个节点进行迁移），迁移的网络速率（因为可能实际到数据磁盘复制）， 通过VMIM也能查看到迁移进度

spec中可以放nodeSelector标签进行迁移

# Windows10 vm持久化部署

### win上传镜像

在创建 VMI 时使用PVC

> PVC是k8s提供的持久化存储方式，当需要对虚拟机变更持久化存储时必须要采用这种方式。笔者在写本文时，kubevirt还未支持blockmode PVC，此章节 仅介绍file方式的。kubevirt中创建虚拟机是以pod空间中的`/disk/`目录下，那么意味着需要将PVC实现进行文件系统格式化，并创建disk/目录将 虚拟机root disk image拷贝至disk目录中。 这个过程可以手动完成比较繁杂，kubevirt提供了新的项目kubevirt/containerized-data-importer自动化这个过程，可参考示例 vm-alpine-datavolume.yaml)通过DataVolume.spec.source定义虚拟操作系统的image路径。

- **一个 PVC 只允许存在一个镜像，只允许一个 VMI 使用，要创建多个 VMI，需要上传多次**

- `/xxxxx.img` 的格式必须是 RAW 格式

  CDI 提供了使用使用 PVC 作为虚拟机磁盘的方案，在虚拟机启动前通过下面方式填充 PVC：

  - 通过 URL 导入虚拟机镜像到 PVC，URL 可以是 http 链接，s3 链接
  - Clone 一个已经存在的 PVC
  - 通过 container registry 导入虚拟机磁盘到 PVC，需要结合 `ContainerDisk` 使用
  - 通过客户端上传本地镜像到 PVC

```
virtctl image-upload   --image-path='Win10x64_2020.iso'   --storage-class csi-rbd-sc   --pvc-name=iso-win10-lfy   --pvc-size=6G   --uploadproxy-url=https://10.99.99.248   --insecure   --wait-secs=240
```

- **--image-path** : 操作系统镜像地址。
- **--pvc-name** : 指定存储操作系统镜像的 PVC，这个 PVC 不需要提前准备好，镜像上传过程中会自动创建。
- **--pvc-size** : PVC 大小，根据操作系统镜像大小来设定，一般略大一个 G 就行。
- **--uploadproxy-url** : cdi-uploadproxy 的 Service IP，可以通过命令 `kubectl -n cdi get svc -l cdi.kubevirt.io=cdi-uploadproxy` 来查看。

```
root@node3:/home/lfy/kubevirt-win10# kubectl -n cdi get svc -l cdi.kubevirt.io=cdi-uploadproxy
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
cdi-uploadproxy   ClusterIP   10.99.99.248   <none>        443/TCP   2d23h
```

PVC一直创建不成功，virtctl显示上传超时。

查看原因：

describe我的pvc后发现：storageclass.storage.k8s.io "csi-rbd-sc" not found

导致

```
Events:
  Type     Reason            Age                  From               Message

----     ------            ----                 ----               -------

  Warning  FailedScheduling  8m55s                default-scheduler  persistentvolumeclaim "iso-win10-lfy-scratch" not found
  Warning  FailedScheduling  86s (x7 over 8m55s)  default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims.
```

寻找资料，Kubernetes StorageClass定义了一个存储类。 可以创建多个StorageClass对象以映射到不同的服务质量级别（即NVMe与基于HDD的池）和功能。

搜索资料发现，csi-rbd-sc应该是ceph与k8s集成后创建的storageclass之一

去掉storageclass的参数后，依然启动不成功，提示

> no persistent volumes available for this claim and no storage class is set

### 解决方案1：

**自创pv去匹配**，无效，判断应该是缺少合适storageclass导致

![image-20210112150357461](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210112150357461.png)

设置hostpath为/home/lfy/windows，我写的基础pv镜像为：

```
kind: PersistentVolume
apiVersion: v1
metadata:
    name: iso-win10-lfy
    labels:
        type: local
spec:
    storageClassName: manual
    capacity:
        storage: 40Gi
    accessModes:
        - ReadWriteOnce
    hostPath:
        path: "/home/lfy/windows"
```

### 解决方案2：

**kubernetes 连接 ceph , 调用 RBD 作为 VMI 数据盘的使用方法**

查阅后找到nfs的解决方案和搭建ceph分布式文件系统的方案，使用网络共享存储如NFS或者ceph来作为系统的镜像盘存放地址，创建特定的storageclass供windows镜像的cdi使用，这样的话在任何一个集群节点都可以运行虚拟机实例。

## 手动上传成功

![image-20210117195509926](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210117195509926.png)

## 使用configmap配置hostdisk特性

```
apiVersion: v1
data:
  feature-gates: LiveMigration,DataVolumes,HostDisk
kind: ConfigMap
metadata:
  labels:
    kubevirt.io: ""
  name: kubevirt-config
  namespace: kubevirt
```

## 配置虚拟机模板文件

win10-vm.yaml：

```
volumes:
      - name: cdromiso
        persistentVolumeClaim:
          claimName: iso-win10-lfy
      - name: harddrive
        hostDisk:
          capacity: 50Gi
          path: /data/disk.img
          type: DiskOrCreate
      - containerDisk:
          image: kubevirt/virtio-container-disk
        name: virtiocontainerdisk
```

3个volume：

- **cdromiso** : 提供操作系统安装镜像，即上文上传镜像后生成的 PVC `iso-win10`。
- **harddrive** : 虚拟机使用的磁盘，即操作系统就会安装在该磁盘上。这里选择 `hostDisk`直接挂载到宿主机以提升性能，如果使用分布式存储则体验非常不好。
- **containerDisk** : 由于 Windows 默认无法识别 raw 格式的磁盘，所以需要安装 virtio 驱动。 containerDisk 可以将打包好 virtio 驱动的容器镜像挂载到虚拟机中。

network字段：

关于网络部分，`spec.template.spec.networks` 定义了一个网络叫 `default`，这里表示使用 Kubernetes 默认的 CNI。`spec.template.spec.domain.devices.interfaces` 选择定义的网络 default，并开启 `masquerade`，以使用网络地址转换 (NAT) 来通过 Linux 网桥将虚拟机连接至 Pod 网络后端。

### （1）尝试使用pvc方式定义一个15G的harddrive模式

https://kubevirt.io/2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html

分别配置名为winhd的pv和pvc，并使其绑定

![image-20210118113549834](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210118113549834.png)

将虚拟机模板文件中的harddrive字段下的配置以pvc替换

      - name: harddrive
        persistentVolumeClaim:
          claimName: winhd
### （2）尝试使用hostdisk，用一个.img来做虚拟硬盘的映射

```
  - name: harddrive
    hostDisk:
      capacity: 50Gi
      path: /data/disk.img
      type: DiskOrCreate
```

## 启动虚拟机

```
kubectl apply -f win10-vm.yaml
virtctl start win10-vm.yaml
```

等待vitl-launcher的pod启动完成和vmi的成功运行

![image-20210118151504516](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210118151504516.png)

## 使用virtVNC

https://github.com/wavezhang/virtVNC，端口映射至31254

![image-20210118161904562](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210118161904562.png)

可以直接在网页端使用类似vnc viewer的功能

![image-20210118162109487](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210118162109487.png)

# Win7部署

win10镜像问题，重配解决win7的启动问题：

上传镜像至PV

![image-20210119133714384](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210119133714384.png)

启动vmi

![image-20210119133750777](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210119133750777.png)

![image-20210119133804097](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210119133804097.png)

![image-20210119133809687](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210119133809687.png)

## 遇到的问题：

1.启动后能正常加载出驱动，但无法安装磁盘，提示硬件不支持问题，在物理机中可以设置bios，可能是win7的问题，windows虚机的选择最好还是Windows10、Windows server R2

2.手动部署虚拟机时，由于单次启动一个虚拟机后会占用一个pvc及其绑定的pv，故每一次上传镜像时定义两个新pv（xxx和xxx-scratch）时，都需在hostpath里重新定义新的hostpath路径，不然无法写入镜像，会遇到如下问题：

> unexpected return value 500, Saving stream failed: Unable to transfer source data to target file: error determining if block device exists: exit status 1, blockdev: ioctl error on BLKGETSIZE64: Inappropriate ioctl for device

![image-20210119134845421](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210119134845421.png)

3.可以先打开若干vm，启动时只是启动vmi实例，那么面临两个问题，首先启动的时候保持最近一次状态（类似物理机重启，不删除vm就不需要重装系统），其次是否可以用操作到一定状态的磁盘镜像映射文件（.img）替换vm对应的.img（制作特殊windows镜像）